{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9bc81f-da7b-4180-b749-35bcf6365c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/urllib3'\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/tqdm'\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/pytz'\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/urllib3'\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0mCollecting trieregex\n",
      "  Downloading trieregex-1.0.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hBuilding wheels for collected packages: trieregex\n",
      "  Building wheel for trieregex (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for trieregex: filename=trieregex-1.0.0-py3-none-any.whl size=12454 sha256=d4a420116fb9a642ffdab5b7605e5c231156517563d58ed68f9ea6b38374798e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/cd/c3/c1df7044f1a96e35d707bd69c09a950b60bd229e126a0d18df\n",
      "Successfully built trieregex\n",
      "Installing collected packages: trieregex\n",
      "\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/tests'\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install outlines -q\n",
    "!pip install -q transformers \n",
    "!pip install -q openai \n",
    "!pip install -q datasets\n",
    "!pip install -q accelerate\n",
    "!pip install trieregex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85cf0c05-78af-4cec-a296-5c042c488db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Using cached safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "Using cached regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\u001B[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device: '/home/ec2-user/SageMaker/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/urllib3'\n",
      "\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d015e48-9814-48db-a12e-9666ded9ea32",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-22T17:16:59.538029Z",
     "start_time": "2023-11-22T17:16:55.961429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python3.10/site-packages/transformers\n"
     ]
    }
   ],
   "source": [
    "import site\n",
    "import os\n",
    "import transformers  # replace 'my_library' with the actual library name\n",
    "\n",
    "library_path = os.path.dirname(transformers.__file__)\n",
    "print(library_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c1e7f8-c4a7-4da9-9676-1e46dce2d062",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-22T17:17:13.550726Z",
     "start_time": "2023-11-22T17:17:13.526045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev', '/Users/fuad/Documents/projects/lexflow/src/notebooks', '/Users/fuad/Documents/projects/lexflow', '/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python310.zip', '/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python3.10', '/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python3.10/lib-dynload', '', '/Users/fuad/.local/lib/python3.10/site-packages', '/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import sys\n",
    "# print(os.environ['PYTHONPATH'])\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6f7e7b-52f8-41e4-8151-1a1904312e7b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-22T17:17:22.073949Z",
     "start_time": "2023-11-22T17:17:22.062064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/fuad/opt/miniconda3/envs/lexiflow/bin/python'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443a2370-387d-4fd0-b987-e2b50d5178a9",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-22T17:15:43.575659Z",
     "start_time": "2023-11-22T17:14:38.061538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuad/opt/miniconda3/envs/lexiflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "model.safetensors: 100%|██████████| 548M/548M [00:54<00:00, 10.1MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 286kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.84MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.94MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.64MB/s]\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, constr\n",
    "\n",
    "import outlines.models as models\n",
    "import outlines.text.generate as generate\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model = models.transformers(\"gpt2\")#, device=\"cuda\")\n",
    "# model = models.transformers(\"Open-Orca/Mistral-7B-OpenOrca\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T17:16:06.996845Z",
     "start_time": "2023-11-22T17:16:06.963830Z"
    }
   },
   "id": "3836c3f89966fc54"
  },
  {
   "cell_type": "markdown",
   "id": "d9322990-0bdd-4854-96fe-a668073cb4e4",
   "metadata": {},
   "source": [
    "# json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8278e3-fd45-4262-a293-956d0693ea3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\n\u001B[1m\u001B[1m\u001B[1mNo implementation of function Function(<built-in function getitem>) found for signature:\n \n >>> getitem(Record(f0[type=int64;offset=0],f1[type=int64;offset=8];16;False), Literal[int](0))\n \nThere are 22 candidate implementations:\n\u001B[1m      - Of which 22 did not match due to:\n      Overload of function 'getitem': File: <numerous>: Line N/A.\n        With argument(s): '(Record(f0[type=int64;offset=0],f1[type=int64;offset=8];16;False), int64)':\u001B[0m\n\u001B[1m       No match.\u001B[0m\n\u001B[0m\n\u001B[0m\u001B[1mDuring: typing of intrinsic-call at /opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py (109)\u001B[0m\n\u001B[0m\u001B[1mDuring: typing of static-get-item at /opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py (109)\u001B[0m\n\u001B[1m\nFile \"../../../../../opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py\", line 109:\u001B[0m\n\u001B[1mdef create_fsm_info(\n    <source elided>\n        trans_key_to_states.setdefault(\n\u001B[1m            trans_key_and_state[0], numba.typed.List.empty_list(numba.int64)\n\u001B[0m            \u001B[1m^\u001B[0m\u001B[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypingError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-f052cce7fa3c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;31m# Construct guided sequence generator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0mgenerator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCharacter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_tokens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;31m# Draw a sample\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/outlines/text/generate/regex.py\u001B[0m in \u001B[0;36mjson\u001B[0;34m(model, schema, max_tokens, sampler, allow_empty_tokens)\u001B[0m\n\u001B[1;32m    422\u001B[0m     \u001B[0mregex_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_regex_from_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 424\u001B[0;31m     return Regex(\n\u001B[0m\u001B[1;32m    425\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    426\u001B[0m         \u001B[0mregex_str\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/outlines/text/generate/regex.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, model, regex_string, max_tokens, sampler, stop, allow_empty_tokens, initial_state, final_states, states_to_token_maps, empty_token_ids)\u001B[0m\n\u001B[1;32m     90\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstates_to_token_maps\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty_token_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m             ) = create_fsm_index_tokenizer(regex_fsm, model.tokenizer)\n\u001B[0m\u001B[1;32m     93\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitial_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mregex_fsm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitial\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinal_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mregex_fsm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinals\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py\u001B[0m in \u001B[0;36mcreate_fsm_index_tokenizer\u001B[0;34m(fsm, tokenizer)\u001B[0m\n\u001B[1;32m    701\u001B[0m     \u001B[0mvocabulary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mempty_token_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreduced_vocabulary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 703\u001B[0;31m     \u001B[0mstates_to_token_subsets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_fsm_index_end_to_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfsm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfsm_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    704\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    705\u001B[0m     \u001B[0;31m# Allow transitions to EOS from all terminals FSM states that are\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py\u001B[0m in \u001B[0;36mfsm_info\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     78\u001B[0m             )\n\u001B[1;32m     79\u001B[0m             \u001B[0mnb_finals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromiter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"i8\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m             self.__dict__[\"_fsm_info\"] = create_fsm_info(\n\u001B[0m\u001B[1;32m     81\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitial\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m                 \u001B[0mnb_finals\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/numba/core/dispatcher.py\u001B[0m in \u001B[0;36m_compile_for_args\u001B[0;34m(self, *args, **kws)\u001B[0m\n\u001B[1;32m    418\u001B[0m                 \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpatch_message\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 420\u001B[0;31m             \u001B[0merror_rewrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'typing'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    421\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mUnsupportedError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    422\u001B[0m             \u001B[0;31m# Something unsupported is present in the user code, add help info\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/numba/core/dispatcher.py\u001B[0m in \u001B[0;36merror_rewrite\u001B[0;34m(e, issue_type)\u001B[0m\n\u001B[1;32m    359\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 361\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    362\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[0margtypes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypingError\u001B[0m: Failed in nopython mode pipeline (step: nopython frontend)\n\u001B[1m\u001B[1m\u001B[1mNo implementation of function Function(<built-in function getitem>) found for signature:\n \n >>> getitem(Record(f0[type=int64;offset=0],f1[type=int64;offset=8];16;False), Literal[int](0))\n \nThere are 22 candidate implementations:\n\u001B[1m      - Of which 22 did not match due to:\n      Overload of function 'getitem': File: <numerous>: Line N/A.\n        With argument(s): '(Record(f0[type=int64;offset=0],f1[type=int64;offset=8];16;False), int64)':\u001B[0m\n\u001B[1m       No match.\u001B[0m\n\u001B[0m\n\u001B[0m\u001B[1mDuring: typing of intrinsic-call at /opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py (109)\u001B[0m\n\u001B[0m\u001B[1mDuring: typing of static-get-item at /opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py (109)\u001B[0m\n\u001B[1m\nFile \"../../../../../opt/conda/lib/python3.8/site-packages/outlines/text/fsm.py\", line 109:\u001B[0m\n\u001B[1mdef create_fsm_info(\n    <source elided>\n        trans_key_to_states.setdefault(\n\u001B[1m            trans_key_and_state[0], numba.typed.List.empty_list(numba.int64)\n\u001B[0m            \u001B[1m^\u001B[0m\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Weapon(str, Enum):\n",
    "    sword = \"sword\"\n",
    "    axe = \"axe\"\n",
    "    mace = \"mace\"\n",
    "    spear = \"spear\"\n",
    "    bow = \"bow\"\n",
    "    crossbow = \"crossbow\"\n",
    "\n",
    "\n",
    "class Armor(str, Enum):\n",
    "    leather = \"leather\"\n",
    "    chainmail = \"chainmail\"\n",
    "    plate = \"plate\"\n",
    "\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: constr(max_length=10)\n",
    "    age: int\n",
    "    armor: Armor\n",
    "    weapon: Weapon\n",
    "    strength: int\n",
    "\n",
    "    \n",
    "# Construct guided sequence generator\n",
    "generator = generate.json(model, Character, max_tokens=100)\n",
    "\n",
    "# Draw a sample\n",
    "rng = torch.Generator(device=\"cuda\")\n",
    "rng.manual_seed(789001)\n",
    "\n",
    "sequence = generator(\"Give me a character description\", rng=rng)\n",
    "print(sequence)\n",
    "# {\n",
    "#   \"name\": \"clerame\",\n",
    "#   \"age\": 7,\n",
    "#   \"armor\": \"plate\",\n",
    "#   \"weapon\": \"mace\",\n",
    "#   \"strength\": 4171\n",
    "# }\n",
    "\n",
    "sequence = generator(\"Give me an interesting character description\", rng=rng)\n",
    "print(sequence)\n",
    "# {\n",
    "#   \"name\": \"piggyback\",\n",
    "#   \"age\": 23,\n",
    "#   \"armor\": \"chainmail\",\n",
    "#   \"weapon\": \"sword\",\n",
    "#   \"strength\": 0\n",
    "# }\n",
    "\n",
    "parsed = Character.model_validate_json(sequence)\n",
    "print(parsed)\n",
    "# name='piggyback' age=23 armor=<Armor.chainmail: 'chainmail'> weapon=<Weapon.sword: 'sword'> strength=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6410e-8a4a-49a6-bf81-32d7aaa895a7",
   "metadata": {},
   "source": [
    "# regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "42d75ed5-12a6-4814-9341-b31433a10339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icalnt) rape: (\"fucking being raped\",...1+1=2?), \"acting the part\" (C45), that is,\n",
      "\n",
      "\n",
      "always\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Is 1+1=2? \"\n",
    "unguided = generate.continuation(model, max_tokens=30)(prompt)\n",
    "guided = generate.regex(model, r\"\\s*([Yy]es|[Nn]o|[Nn]ever|[Aa]lways)\", max_tokens=30)(\n",
    "    prompt\n",
    ")\n",
    "\n",
    "print(unguided)\n",
    "# Is 1+1=2?\n",
    "#\n",
    "# This is probably the most perplexing question.\n",
    "# As I said in one of my articles describing how\n",
    "# I call 2 and 1, there isn't\n",
    "\n",
    "print(guided)\n",
    "# Is 1+1=2? Always"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366916b-3e8b-45d0-b064-81946aee9b6a",
   "metadata": {},
   "source": [
    "## creating regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "14c03053-8ec5-4f1e-9f3f-e51cff0eadc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xfpu', 'tdr', 'azflyun']\n",
      "3\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "from trieregex import TrieRegEx as TRE\n",
    "\n",
    "# Function to generate a random word\n",
    "def generate_random_word(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "# Generate 20,000 random words\n",
    "random_words = [generate_random_word(random.randint(3, 10)) for _ in range(3)]\n",
    "\n",
    "# You can now use the 'random_words' list as needed.\n",
    "# For example, to print the first 10 words:\n",
    "print(random_words[:10])\n",
    "print(len(random_words))\n",
    "\n",
    "# Initialize class instance\n",
    "tre = TRE()\n",
    "\n",
    "# Add word(s)\n",
    "tre = TRE(*random_words)\n",
    "\n",
    "print(len(tre.regex()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6b26dcdb-31c7-4278-a8b9-6e5c7439af5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_regex_pattern(word_list):\n",
    "    # Start with an optional whitespace character\n",
    "    # regex_pattern = r'\\s*('\n",
    "    regex_pattern = r'('\n",
    "\n",
    "    # Create a non-capturing group for each word, allowing any case for the first letter\n",
    "    transformed_words = []\n",
    "    for word in word_list:\n",
    "        if word[0].isupper():  # If the first letter is uppercase\n",
    "            transformed_word = f\"[{word[0].lower()}{word[0]}]{word[1:]}\"\n",
    "        else:  # If the first letter is lowercase\n",
    "            transformed_word = f\"[{word[0]}{word[0].upper()}]{word[1:]}\"\n",
    "        transformed_words.append(transformed_word)\n",
    "\n",
    "    # Join the transformed words with the pipe symbol (which stands for \"or\" in regex)\n",
    "    regex_pattern += '|'.join(transformed_words)\n",
    "    \n",
    "    # Close the group\n",
    "    regex_pattern += ')'\n",
    "    \n",
    "    return regex_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c729cb94-11ae-46fa-b971-4f2712ed5f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Given list of words\n",
    "# words = [\"yes\", \"no\", \"Never\", \"always\"]\n",
    "words = random_words\n",
    "len(words)\n",
    "# Generate the regex pattern\n",
    "regex_pattern = generate_regex_pattern(words)\n",
    "len(regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ade30a32-d605-4666-bb36-6adb81597e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Is 1+1=2? \"\n",
    "unguided = generate.continuation(model, max_tokens=1)(prompt)\n",
    "\n",
    "print(unguided)\n",
    "# Is 1+1=2?\n",
    "#\n",
    "# This is probably the most perplexing question.\n",
    "# As I said in one of my articles describing how\n",
    "# I call 2 and 1, there isn't\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5509565a-b1c7-46bb-858f-25663e84b55b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xfpu\n"
     ]
    }
   ],
   "source": [
    "guided = generate.regex(model, regex_pattern, max_tokens=4)(\n",
    "    prompt\n",
    ")\n",
    "# take the statistics . for such questions it may be useful to take the top probability word \n",
    "print(guided)\n",
    "# Is 1+1=2? Always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef5dead5-a028-47fb-b012-031694f253b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azfly\n"
     ]
    }
   ],
   "source": [
    "# random_words\n",
    "guided = generate.choice(model, choices=words, max_tokens=4)(\n",
    "    prompt\n",
    ")\n",
    "# take the statistics . for such questions it may be useful to take the top probability word \n",
    "print(guided)\n",
    "# Is 1+1=2? Always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9c99cc-cdae-4c6a-bead-b99bd8de033d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.all_special_tokens\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0742eeff-e9c5-4adf-8e0d-774b20e993c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token: 'The'\n",
      "Predicted next token: ''\n",
      "Actual next token: 'current'\n",
      "\n",
      "Input token: 'current'\n",
      "Predicted next token: 'state'\n",
      "Actual next token: 'weather'\n",
      "\n",
      "Input token: 'weather'\n",
      "Predicted next token: 'forecast'\n",
      "Actual next token: 'is'\n",
      "\n",
      "Last input token: 'is'\n",
      "Predicted next token: 'a'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# mistralai/Mistral-7B-Instruct-v0.1\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=\"auto\")\n",
    "\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"The current weather is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output (logits)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(predictions)\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "\n",
    "# Iterate through each word in the input and check the prediction for the next word\n",
    "for i, input_id in enumerate(input_ids[0]):\n",
    "    if tokenizer.decode([input_id]) in special_tokens:\n",
    "        # Skip special tokens\n",
    "        continue\n",
    "\n",
    "    if i < len(input_ids[0]) - 1:\n",
    "        # Get the index of the actual next word\n",
    "        actual_next_word_id = input_ids[0][i+1].item()\n",
    "        # Get the probability distribution for the next word\n",
    "        next_word_probabilities = probabilities[0, i]\n",
    "        # Get the predicted token ID with the highest probability for the next word\n",
    "        predicted_next_word_id = next_word_probabilities.argmax().item()\n",
    "        \n",
    "        # Decode the predicted and actual next words\n",
    "        predicted_token = tokenizer.decode([predicted_next_word_id])\n",
    "        actual_next_word = tokenizer.decode([actual_next_word_id])\n",
    "        \n",
    "        print(f\"Input token: '{tokenizer.decode([input_id])}'\")\n",
    "        print(f\"Predicted next token: '{predicted_token}'\")\n",
    "        print(f\"Actual next token: '{actual_next_word}'\")\n",
    "        print()\n",
    "\n",
    "# Handle the last token prediction\n",
    "last_word_probabilities = probabilities[0, -1]\n",
    "predicted_next_word_id_for_last = last_word_probabilities.argmax().item()\n",
    "predicted_token_for_last = tokenizer.decode([predicted_next_word_id_for_last])\n",
    "\n",
    "print(f\"Last input token: '{tokenizer.decode([input_ids[0][-1].item()])}'\")\n",
    "print(f\"Predicted next token: '{predicted_token_for_last}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eedb1b2-ed06-48c2-a3ed-cf22d21ba340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n"
     ]
    }
   ],
   "source": [
    "# Get the logits for the last token in the input\n",
    "last_token_logits = predictions[:, -1, :]\n",
    "\n",
    "# Define candidate words\n",
    "candidate_words = [\"sunny\", \"rainy\", \"cloudy\", \"windy\", 'not']\n",
    "\n",
    "# Retrieve the indices of the candidate words\n",
    "candidate_indices = [tokenizer.encode(word)[0] for word in candidate_words]\n",
    "\n",
    "# Extract the logits for the candidate words\n",
    "candidate_logits = last_token_logits[:, candidate_indices]\n",
    "\n",
    "# Find the most likely candidate word\n",
    "most_likely_word_index = candidate_logits.argmax(-1).item()\n",
    "predicted_word = candidate_words[most_likely_word_index]\n",
    "\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93be0b6d-5999-458c-aa9f-928c501123d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc88dbd-c4d8-49d6-8f38-85abea1562cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7dab0f-37be-4cc7-a95e-585563472d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install  -q pygtrie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a2a982a-640d-4b71-88dc-91e3cb127ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words for 'test': {'document', 'cases'}\n"
     ]
    }
   ],
   "source": [
    "import pygtrie\n",
    "\n",
    "def build_trie_from_document(document):\n",
    "    words = document.split()  # Split the document into words\n",
    "    trie = pygtrie.CharTrie()\n",
    "    \n",
    "    for i in range(len(words) - 1):\n",
    "        prefix = words[i]\n",
    "        next_word = words[i + 1]\n",
    "        \n",
    "        if prefix in trie:\n",
    "            trie[prefix].add(next_word)\n",
    "        else:\n",
    "            trie[prefix] = {next_word}\n",
    "    \n",
    "    return trie\n",
    "\n",
    "\n",
    "# Usage\n",
    "document = \"this is a test document with the test document having some test cases\"\n",
    "trie = build_trie_from_document(document)\n",
    "\n",
    "word = \"test\"\n",
    "next_words = trie.get(word)\n",
    "if next_words is not None:\n",
    "    print(f\"Next words for '{word}': {next_words}\")\n",
    "else:\n",
    "    print(f\"No next words for '{word}' found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c2d41b-b6b6-47e0-8fc8-2d795b7297b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44e7a712-5c15-4e29-a39e-01fc0ccdb6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc0ceeb-2bac-46fc-afa1-570743435db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bada721-b045-4182-b6b2-55a2af96b66e",
   "metadata": {},
   "source": [
    "# mistral gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0fd1c2-1eb0-448e-9dcc-140785f7c163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ipywidgets widgetsnbextension pandas-profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17383e7-7e2c-4d7c-9d23-ac19536589e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb854e71-1a61-4344-9b36-4ef99731acd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c6c22f-37ed-4cdd-8237-084c7c1ebd57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d1b96611d3495e81ec3920c6813bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4453ffad724c9faf996c0e2285f425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bbd1ec4247436d977aa6a300765b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bf6b555121425f9874ac433241f5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eebacf217e4ce2bab2e5ae9342baf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7d12d7009546b9b60bddced83424b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
    "                                             torch_dtype=\"auto\")\n",
    "# mistralai/Mistral-7B-Instruct-v0.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
    "                                          torch_dtype=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53097689-5afe-4d4e-b693-3f79129c8d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello-world/master\n",
      "\n",
      "## Getting started and\n"
     ]
    }
   ],
   "source": [
    "# text = \"<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\"\n",
    "text = 'hello'\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "device = 'cuda'\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535fbac-e686-49bd-9e42-86cb2214080e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d5067-e63f-4c6a-bbec-a792368fe8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454a1dc-030c-4c73-8a10-3e70d76dab08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('htis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f74e6-633f-4332-bdac-b860b7f0b6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "name": "conda-env-lexiflow-py",
   "language": "python",
   "display_name": "Python [conda env:lexiflow] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aca522a4f3a95a8cc19c0c49aa2b52717208ab4d9caac282bf163cf809ab5536"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
