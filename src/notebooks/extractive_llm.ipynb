{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4366916b-3e8b-45d0-b064-81946aee9b6a",
   "metadata": {},
   "source": [
    "# HuggingFace TrieGuided Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df81925-4a37-4b03-8a68-1955d6205ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open-Orca/Mistral-7B-OpenOrca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4419fb97-221c-4238-859e-119effe9fbdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3692293-74a4-4727-8ded-a9ed00249c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103c1a64-3d79-49a8-a445-0df9170df7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trieregex\n",
      "  Using cached trieregex-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: trieregex\n",
      "Successfully installed trieregex-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install outlines -q\n",
    "!pip install -q transformers \n",
    "!pip install -q openai \n",
    "!pip install -q datasets\n",
    "!pip install -q accelerate\n",
    "!pip install trieregex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75fcab6-093f-4501-91ae-01e08ba6d1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/.cs/conda/envs/codeserver_py39/lib/python3.9/site-packages/transformers\n"
     ]
    }
   ],
   "source": [
    "import site\n",
    "import os\n",
    "import transformers  # replace 'my_library' with the actual library name\n",
    "\n",
    "library_path = os.path.dirname(transformers.__file__)\n",
    "print(library_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85851a2f-4c6a-4c13-a016-e87221d46cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: jupyterlab: command not found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.18 ('codeserver_py39')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n codeserver_py39 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!jupyterlab --ContentsManager.allow_hidden=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0b6c4-d6d9-4e9a-9b91-bf8899ffbe64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/projects/lexflow/src/notebooks', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg', '/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg', '/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/root/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50c282-71ce-409d-80a0-29c1337b50a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b81be8-241d-401f-bc5a-9cfe71ca8f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc3ffc-e2a3-4539-b4b4-e0199273ba65",
   "metadata": {},
   "source": [
    "##Â Exploring api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28327965-1c4e-4eb7-8036-5734595fbb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f3768abc6a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fe4f82-6c64-46a5-970f-12cbfc61eecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"eos_token_id\": 50256\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcefbb5-be94-45e5-9624-d015af7df5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "class CustomLogitsProcessor(LogitsProcessor):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # Modify the scores here according to your custom function\n",
    "        return modified_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a251fe-696d-458d-b037-ef887fa03c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "\n",
    "logit_processors = LogitsProcessorList([CustomLogitsProcessor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e6c4d5-86bb-4d42-9f0c-0a037f821498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4ba331-4a1f-48e7-8605-0d3606e07ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.35.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc22afa-508e-4377-b224-3ec5fef94996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['logit_processors'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fac5924c2308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_processors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogit_processors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# All unused kwargs must be model kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0;31m# 2. Set generation parameters if not already defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munused_model_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1263\u001b[0m                 \u001b[0;34mf\"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;34m\" generate arguments will also show up in this list)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['logit_processors'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "model.generate(**input_ids, logit_processors=logit_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd31266f-988e-443f-8b8c-57544777bc15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I also like to play with my dog.\n",
      "\n",
      "How do you feel about your dog?\n",
      "\n",
      "I love to play with my dog. I love to play with my dog. I\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b1d1ef-0686-4860-a4d4-62380b4f67da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a699611-f2c2-4bc3-9860-5c5cbba7726e",
   "metadata": {},
   "source": [
    "## testing huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0742eeff-e9c5-4adf-8e0d-774b20e993c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 13:23:06.045 pytorch-1-10-cpu-py38-ml-t3-medium-7bde6515cda8eba7f7394af8b8e0:1272 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 13:23:06.403 pytorch-1-10-cpu-py38-ml-t3-medium-7bde6515cda8eba7f7394af8b8e0:1272 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Input word: 'The'\n",
      "Predicted next word: '\n",
      "'\n",
      "Actual next word: ' current'\n",
      "\n",
      "Input word: ' current'\n",
      "Predicted next word: ' state'\n",
      "Actual next word: ' weather'\n",
      "\n",
      "Input word: ' weather'\n",
      "Predicted next word: ' is'\n",
      "Actual next word: ' is'\n",
      "\n",
      "Input word: ' is'\n",
      "Predicted next word: ' not'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"The current weather is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output (logits)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(predictions)\n",
    "\n",
    "# Iterate through each word in the input and check the prediction for the next word\n",
    "for i in range(len(input_ids[0]) - 1):\n",
    "    # Get the index of the actual next word\n",
    "    actual_next_word_id = input_ids[0][i+1].item()\n",
    "    # Get the probability distribution for the next word\n",
    "    next_word_probabilities = probabilities[0, i]\n",
    "    # Get the predicted token ID with the highest probability for the next word\n",
    "    predicted_next_word_id = next_word_probabilities.argmax().item()\n",
    "    \n",
    "    # Decode the predicted and actual next words\n",
    "    predicted_token = tokenizer.decode([predicted_next_word_id])\n",
    "    actual_next_word = tokenizer.decode([actual_next_word_id])\n",
    "    \n",
    "    print(f\"Input word: '{tokenizer.decode([input_ids[0][i].item()])}'\")\n",
    "    print(f\"Predicted next word: '{predicted_token}'\")\n",
    "    print(f\"Actual next word: '{actual_next_word}'\")\n",
    "    print()\n",
    "\n",
    "# Now handle the last token separately since there's no actual next word in the input\n",
    "last_word_probabilities = probabilities[0, -1]\n",
    "predicted_next_word_id_for_last = last_word_probabilities.argmax().item()\n",
    "predicted_token_for_last = tokenizer.decode([predicted_next_word_id_for_last])\n",
    "\n",
    "print(f\"Input word: '{tokenizer.decode([input_ids[0][-1].item()])}'\")\n",
    "print(f\"Predicted next word: '{predicted_token_for_last}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145fc1a3-e69c-4f42-a0fd-e347f4054227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50257])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eedb1b2-ed06-48c2-a3ed-cf22d21ba340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunny\n"
     ]
    }
   ],
   "source": [
    "# Get the logits for the last token in the input\n",
    "last_token_logits = predictions[:, -1, :]\n",
    "\n",
    "# Define candidate words\n",
    "candidate_words = [\"sunny\", \"rainy\", \"cloudy\", \"windy\"]\n",
    "\n",
    "# Retrieve the indices of the candidate words\n",
    "candidate_indices = [tokenizer.encode(word)[0] for word in candidate_words]\n",
    "\n",
    "# Extract the logits for the candidate words\n",
    "candidate_logits = last_token_logits[:, candidate_indices]\n",
    "\n",
    "# Find the most likely candidate word\n",
    "most_likely_word_index = candidate_logits.argmax(-1).item()\n",
    "predicted_word = candidate_words[most_likely_word_index]\n",
    "\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93be0b6d-5999-458c-aa9f-928c501123d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc88dbd-c4d8-49d6-8f38-85abea1562cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c58191-4834-478a-b53f-14ebce1ef854",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using it on document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa72f48-1292-472f-a795-abc396e871dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_document(file_path):\n",
    "    \"\"\"\n",
    "    Reads and returns the contents of a text file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the text file to be read.\n",
    "\n",
    "    Returns:\n",
    "    str: The contents of the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Example usage:\n",
    "document = read_document('document.txt')\n",
    "# print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e220f0c9-6a9d-4994-8519-055baaaada99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suella Bra'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41db5f52-e3e1-427a-a0bc-0817e3181d74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'claim', 'accuses', 'for', '-', 'said', 'was', 'has'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_word_graph(document):\n",
    "    # Using defaultdict to automatically handle missing keys\n",
    "    graph = defaultdict(set)\n",
    "    previous_word = None\n",
    "    # Loop over each word in the document\n",
    "    for word in document.split():\n",
    "        # Skip adding the first word to any previous word\n",
    "        if previous_word is not None:\n",
    "            # Add the word to the set of words that come after the previous word\n",
    "            graph[previous_word].add(word)\n",
    "        previous_word = word  # Update the previous word\n",
    "    return graph\n",
    "\n",
    "# Example usage\n",
    "# document = \"hello my name is hello his name is ....\"\n",
    "word_graph = build_word_graph(document)\n",
    "\n",
    "# Now you can query the graph for what comes after \"hello\"\n",
    "print(word_graph[\"Braverman\"])  # Outputs: {'my', 'his'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f1696e2-70c5-413c-863c-870792edcd70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(TrieNode)\n",
    "        self.next_words = defaultdict(int)  # Stores next words and their frequencies\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, tokens):\n",
    "        # Insert all subsequences of tokens into the trie\n",
    "        for i in range(len(tokens)):\n",
    "            current = self.root\n",
    "            for j in range(i, len(tokens)):\n",
    "                current = current.children[tokens[j]]\n",
    "                if j + 1 < len(tokens):\n",
    "                    current.next_words[tokens[j + 1]] += 1  # Increment the count of the next word\n",
    "\n",
    "    def search_next_words(self, sequence):\n",
    "        # Find the next possible words after the sequence of tokens\n",
    "        current = self.root\n",
    "        for token in sequence:\n",
    "            if token not in current.children:\n",
    "                return {}  # If the sequence is not found, return an empty dictionary\n",
    "            current = current.children[token]\n",
    "        return current.next_words\n",
    "\n",
    "# Example usage:\n",
    "trie = Trie()\n",
    "\n",
    "# Tokenize the text and insert into the trie\n",
    "# Assuming 'text' is a string containing the full document\n",
    "# text = \"\"\"\n",
    "# hello there how are you\n",
    "# hello there how have you been\n",
    "# hello there who are you\n",
    "# hello there she is my friend\n",
    "# hello there he is a colleague\n",
    "# \"\"\"\n",
    "text = document\n",
    "tokens = text.split()\n",
    "trie.insert(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b89b438-b4b3-4b1b-95dc-8ac22c359eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'failed': 1})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the next words after the sequence \"hello there\"\n",
    "sequence = \"Mrs Braverman said Mr Sunak had\".split()\n",
    "next_words = trie.search_next_words(sequence)\n",
    "next_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce90121-37cf-47b4-ac9a-955c206f88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f516dd94-6df0-4d1d-87db-2843d8b56ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode the input text\n",
    "input_text = \"Extract from the text what Mrs Braverman said\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output (logits)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "probabilities = softmax(predictions)\n",
    "\n",
    "# Now handle the last token separately since there's no actual next word in the input\n",
    "last_word_probabilities = probabilities[0, -1]\n",
    "last_token_logits = predictions[:, -1, :]\n",
    "\n",
    "# Define the trie with potential next words\n",
    "# trie = # (Assume this is your trie object which has the method search_next_words(sequence))\n",
    "\n",
    "# Get the possible next words from the trie given the sequence of tokens\n",
    "sequence_of_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "next_words_from_trie = trie.search_next_words(sequence_of_tokens)\n",
    "print(next_words_from_trie)\n",
    "# Convert these possible next words to their corresponding token IDs\n",
    "next_word_ids_from_trie = {tokenizer.encode(word)[0] for word in next_words_from_trie}\n",
    "\n",
    "# Mask the probabilities\n",
    "masked_probabilities = last_word_probabilities.clone()  # Clone the tensor to avoid modifying the original probabilities\n",
    "for idx in range(masked_probabilities.size(0)):\n",
    "    if idx not in next_word_ids_from_trie:\n",
    "        masked_probabilities[idx] = 0\n",
    "\n",
    "# Renormalize the probabilities\n",
    "masked_probabilities /= masked_probabilities.sum()\n",
    "\n",
    "# Find the most likely word from the trie\n",
    "most_likely_word_id = masked_probabilities.argmax().item()\n",
    "predicted_word = tokenizer.decode([most_likely_word_id])\n",
    "\n",
    "print(predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2cdf20c2-ac40-40a6-9ef4-fa202ed23688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the function to predict the next word using a trie for restrictions\n",
    "def predict_next_word(input_text, trie=None, use_trie=True):\n",
    "    \"\"\"\n",
    "    Predicts the next word using GPT-2 model with an option to restrict predictions based on a trie.\n",
    "    \n",
    "    Parameters:\n",
    "    input_text (str): The input text string for which we want to predict the next word.\n",
    "    trie (Trie object): An optional trie object for restricting predictions.\n",
    "    use_trie (bool): A flag to determine whether to use the trie for restricting predictions.\n",
    "    \n",
    "    Returns:\n",
    "    str: The predicted next word.\n",
    "    \"\"\"\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Get the model's output (logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    probabilities = softmax(predictions)\n",
    "\n",
    "    # Get the logits and probabilities for the last token in the input\n",
    "    last_token_logits = predictions[:, -1, :]\n",
    "    last_word_probabilities = probabilities[0, -1]\n",
    "\n",
    "    # If the trie is provided and we're using it to restrict predictions\n",
    "    if trie and use_trie:\n",
    "        # Get the sequence of tokens from the input text\n",
    "        sequence_of_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        \n",
    "        # Get the possible next words from the trie given the sequence of tokens\n",
    "        next_words_from_trie = trie.search_next_words(sequence_of_tokens)\n",
    "        \n",
    "        # Convert these possible next words to their corresponding token IDs\n",
    "        next_word_ids_from_trie = {tokenizer.encode(word, add_prefix_space=True)[0] for word in next_words_from_trie}\n",
    "\n",
    "        # Mask the probabilities for tokens not in the trie's next words\n",
    "        mask = torch.full_like(last_word_probabilities, fill_value=float('-inf'))\n",
    "        for idx in next_word_ids_from_trie:\n",
    "            mask[idx] = 0\n",
    "        masked_logits = last_token_logits + mask\n",
    "\n",
    "        # Apply softmax to convert masked logits to probabilities\n",
    "        masked_probabilities = softmax(masked_logits)\n",
    "    else:\n",
    "        masked_probabilities = last_word_probabilities\n",
    "\n",
    "    # Get the token ID with the highest probability\n",
    "    most_likely_word_id = masked_probabilities.argmax().item()\n",
    "    # Decode the token ID to get the word\n",
    "    predicted_word = tokenizer.decode([most_likely_word_id])\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "predicted_word = predict_next_word(document[:512] + \"Extract from the text what Mrs Braverman said:'\", trie, use_trie=True)\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd29d2e-a0ca-46e9-b5c2-e66fa676a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gotta do some proper decoding\n",
    "# why am i getting ! it should always be one of the words\n",
    "# maybe there is a need for fine tuning"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.18 ('codeserver_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8dfa52194188e48d6802840b718748a7140cbefa937517cab1c2b201bcac497"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
